{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "extra_path = open(\"/home/developer/symforce.env\", \"r\").read().strip()\n",
    "if extra_path not in sys.path:\n",
    "    sys.path.append(extra_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import symforce\n",
    "\n",
    "# symforce.set_symbolic_api(\"symengine\")\n",
    "symforce.set_log_level(\"warning\")\n",
    "\n",
    "# Set epsilon to a symbol for safe code generation.  For more information, see the Epsilon tutorial:\n",
    "# https://symforce.org/tutorials/epsilon_tutorial.html\n",
    "# symforce.set_epsilon_to_symbol()\n",
    "# symforce.set_epsilon_to_number(1e-10)\n",
    "symforce.set_epsilon_to_zero()\n",
    "\n",
    "import symforce.symbolic as sf\n",
    "from symforce import dataclass\n",
    "from symforce.notebook_util import set_notebook_defaults\n",
    "\n",
    "set_notebook_defaults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CameraConfig:\n",
    "    t_x: float\n",
    "    t_y: float\n",
    "    t_z: float\n",
    "    yaw: float\n",
    "    pitch: float\n",
    "    roll: float\n",
    "    f_x: float = 100\n",
    "    f_y: float = 100\n",
    "    c_x: float = 250\n",
    "    c_y: float = 250\n",
    "    width: int = 500\n",
    "    height: int = 500\n",
    "\n",
    "\n",
    "def get_camera_calibration_model(config: CameraConfig) -> sf.CameraCal:\n",
    "    return sf.SphericalCameraCal(\n",
    "        focal_length=(config.f_x, config.f_y),\n",
    "        principal_point=(config.c_x, config.c_y),\n",
    "        distortion_coeffs=(0.1, 0.005, 0.02, 0.02, 0.0, 0.0),\n",
    "        critical_theta=np.deg2rad(170),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_camera_image_size(config: CameraConfig) -> Tuple[int, int]:\n",
    "    return (config.width, config.height)\n",
    "\n",
    "\n",
    "def get_camera_pose(config: CameraConfig) -> sf.Pose3:\n",
    "    return sf.Pose3(\n",
    "        R=sf.Rot3.from_yaw_pitch_roll(config.yaw, config.pitch, config.roll),\n",
    "        t=sf.V3(config.t_x, config.t_y, config.t_z),\n",
    "    )\n",
    "\n",
    "\n",
    "def create_camera_from_config(config: CameraConfig) -> sf.PosedCamera:\n",
    "    return sf.PosedCamera(\n",
    "        pose=get_camera_pose(config),\n",
    "        calibration=get_camera_calibration_model(config),\n",
    "        image_size=get_camera_image_size(config),\n",
    "    )\n",
    "\n",
    "\n",
    "class VehicleSensors:\n",
    "    def __init__(self):\n",
    "        self.camera_configs = {\n",
    "            \"front\": CameraConfig(\n",
    "                t_x=2.5,\n",
    "                t_y=0.0,\n",
    "                t_z=0.5,\n",
    "                yaw=-math.pi / 2,\n",
    "                pitch=0.0,\n",
    "                roll=-math.pi / 2,\n",
    "            ),\n",
    "            \"left\": CameraConfig(\n",
    "                t_x=1.0, t_y=1.0, t_z=0.5, yaw=0.0, pitch=0.0, roll=-math.pi / 2\n",
    "            ),\n",
    "            \"right\": CameraConfig(\n",
    "                t_x=1.0, t_y=-1.0, t_z=0.5, yaw=math.pi, pitch=0.0, roll=-math.pi / 2\n",
    "            ),\n",
    "            \"rear\": CameraConfig(\n",
    "                t_x=-0.5,\n",
    "                t_y=0.0,\n",
    "                t_z=0.5,\n",
    "                yaw=math.pi / 2,\n",
    "                pitch=0.0,\n",
    "                roll=-math.pi / 2,\n",
    "            ),\n",
    "        }\n",
    "        self.cameras = {\n",
    "            cid: create_camera_from_config(cfg)\n",
    "            for cid, cfg in self.camera_configs.items()\n",
    "        }\n",
    "\n",
    "\n",
    "class Vehicle:\n",
    "    def __init__(self, pose: sf.Pose3):\n",
    "        self.sensors = VehicleSensors()\n",
    "        # vehicle pose in the world frame, world_T_body\n",
    "        self.pose = pose\n",
    "\n",
    "    def _project_world_point_to_camera(\n",
    "        self, camera: sf.PosedCamera, world_point: sf.V3\n",
    "    ) -> Tuple[sf.V2, bool]:\n",
    "        body_T_world = self.pose.inverse()\n",
    "        camera_T_body = camera.pose.inverse()\n",
    "        return camera.pixel_from_camera_point(\n",
    "            camera_T_body * body_T_world * world_point\n",
    "        )\n",
    "\n",
    "    def project_world_point_to_camera(\n",
    "        self, camera_id: str, world_point: sf.V3\n",
    "    ) -> Tuple[sf.V2, bool]:\n",
    "        camera = self.sensors.cameras[camera_id]\n",
    "        return self._project_world_point_to_camera(camera, world_point)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Observation:\n",
    "    landmark_id: int\n",
    "    u: float\n",
    "    v: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class VehiclePerceptions:\n",
    "    front: list[Observation] = field(default_factory=list)\n",
    "    left: list[Observation] = field(default_factory=list)\n",
    "    right: list[Observation] = field(default_factory=list)\n",
    "    rear: list[Observation] = field(default_factory=list)\n",
    "\n",
    "\n",
    "class World:\n",
    "    def __init__(self):\n",
    "        self._world_points = np.indices((11, 11), dtype=float).reshape(2, -1).T\n",
    "        self._perceptions = VehiclePerceptions()\n",
    "\n",
    "    @property\n",
    "    def points(self) -> np.ndarray:\n",
    "        return self._world_points\n",
    "\n",
    "    @property\n",
    "    def perceptions(self) -> VehiclePerceptions:\n",
    "        return self._perceptions\n",
    "\n",
    "    def get_perceptions_array(self, camera_id: str) -> np.ndarray:\n",
    "        return np.array([[o.u, o.v] for o in getattr(self._perceptions, camera_id)])\n",
    "\n",
    "    def gen_perceptions(self, vehicle: Vehicle) -> dict:\n",
    "        for point_id, world_point in enumerate(self._world_points):\n",
    "            for camera_id in [\"front\", \"left\", \"right\", \"rear\"]:\n",
    "                camera_pixel, is_visible = vehicle.project_world_point_to_camera(\n",
    "                    camera_id, sf.V3(world_point[0], world_point[1], 0.0)\n",
    "                )\n",
    "                if bool(is_visible):\n",
    "                    getattr(self._perceptions, camera_id).append(\n",
    "                        Observation(\n",
    "                            point_id, float(camera_pixel[0]), float(camera_pixel[1])\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "\n",
    "world = World()\n",
    "\n",
    "vehicle = Vehicle(\n",
    "    sf.Pose3(R=sf.Rot3.from_yaw_pitch_roll(0.0, 0.0, 0.0), t=sf.V3(4.0, 0.0, 0.0))\n",
    ")\n",
    "\n",
    "world.gen_perceptions(vehicle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(8, 8))\n",
    "for i, camera_id in enumerate([\"front\", \"left\", \"right\", \"rear\"]):\n",
    "    multi_index = np.unravel_index(i, ax.shape)\n",
    "    observations_uv = world.get_perceptions_array(camera_id)\n",
    "    if len(observations_uv):\n",
    "        ax[multi_index].scatter(\n",
    "            observations_uv[:, 0], observations_uv[:, 1], s=1, c=\"red\"\n",
    "        )\n",
    "    ax[multi_index].set(xlim=(0, 500), ylim=(0, 500), title=camera_id)\n",
    "    ax[multi_index].invert_yaxis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pytransform3d.transformations import plot_transform\n",
    "\n",
    "\n",
    "# Helper to convert Pose3 to a 4x4 homogeneous matrix.\n",
    "def pose3_to_homogeneous(pose: sf.Pose3) -> np.ndarray:\n",
    "    T = np.eye(4)\n",
    "    T[:3, :3] = (\n",
    "        pose.R.to_rotation_matrix().to_numpy()\n",
    "    )  # Assuming pose.R is a 3x3 rotation matrix.\n",
    "    T[:3, 3] = pose.t  # Assuming pose.t is a 3-element translation vector.\n",
    "    return T\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "# Plot the vehicle's body frame in world coordinates.\n",
    "T_body_world = pose3_to_homogeneous(vehicle.pose)\n",
    "# plot_transform(T_body_world, ax=ax, s=1.0, label=\"Body Frame\")\n",
    "plot_transform(ax, T_body_world, s=2.0)\n",
    "\n",
    "for i, camera_id in enumerate([\"front\", \"left\", \"right\", \"rear\"]):\n",
    "    cam = vehicle.sensors.cameras[camera_id]\n",
    "    T_cam_body = pose3_to_homogeneous(cam.pose)\n",
    "    T_cam_world = (\n",
    "        T_body_world @ T_cam_body\n",
    "    )  # world_T_camera = world_T_body * body_T_camera\n",
    "    plot_transform(ax, T_cam_world, s=1.0)\n",
    "\n",
    "ax.set(xlim=(-2, 10), ylim=(-2, 10), zlim=(-2, 10))\n",
    "ax.scatter(\n",
    "    world.points[:, 0], world.points[:, 1], np.zeros_like(world.points[:, 0]), s=1\n",
    ")\n",
    "ax.axis(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # body w.r.t. world frame\n",
    "# world_T_body = sf.Pose3.symbolic(\"world_T_body\")\n",
    "# # odometry\n",
    "# body_T_body_next = sf.Pose3.symbolic(\"body_T_body_next\")\n",
    "\n",
    "# world_T_body_next = world_T_body * body_T_body_next\n",
    "\n",
    "# # camera w.r.t. body frame\n",
    "# body_T_camera = sf.Pose3.symbolic(\"body_T_camera\")\n",
    "\n",
    "# # camera calibration\n",
    "# camera_calibration = sf.LinearCameraCal.symbolic(\"camera_calibration\")\n",
    "\n",
    "# # world point\n",
    "# world_point = sf.V3.symbolic(\"world_point\")\n",
    "\n",
    "# # project world point to image\n",
    "# body_T_world = world_T_body_next.inverse()\n",
    "# camera_T_body = body_T_camera.inverse()\n",
    "# camera_point = camera_T_body * body_T_world * world_point\n",
    "# image_point, is_valid = camera_calibration.project(camera_point)\n",
    "\n",
    "# # now we have world points transformed to image points\n",
    "# # and we also have the validity of the projection\n",
    "# # we also know the current detections in the image\n",
    "# # so we do matching and then we can define the loss\n",
    "# print(is_valid)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "development",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
